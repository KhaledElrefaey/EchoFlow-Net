{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3w0LnCIHn0Ht",
      "metadata": {
        "id": "3w0LnCIHn0Ht"
      },
      "source": [
        "---\n",
        "\n",
        "# EchoFlow-Net: Automated Ejection Fraction Estimation from Echocardiographic Segmentation using Deep Learning and Calibration\n",
        "**Author:** Elrefaey, K. M. E. (MBBS)  \n",
        "**Journal Target:** Frontiers in Cardiovascular Medicine  \n",
        "**Date:** November 2025\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RK_xW9gR6bta",
      "metadata": {
        "id": "RK_xW9gR6bta"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 1 : Set up dependencies\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdohbWhBmPli",
      "metadata": {
        "id": "cdohbWhBmPli"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "compliant-gossip",
      "metadata": {
        "id": "compliant-gossip"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 2 : Imports And Seed everything\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "0F7rW6SNTn5r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F7rW6SNTn5r",
        "outputId": "786b12ef-f660-4ba5-cbdc-588565745cf8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch: 2.8.0+cu126\n",
            "Albumentations: 2.0.8\n",
            "SMP: 0.5.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import random\n",
        "import os\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import helper\n",
        "from pathlib import Path\n",
        "import albumentations as A\n",
        "import nibabel as nib\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import segmentation_models_pytorch as smp\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import re\n",
        "from math import pi\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.model_selection import KFold\n",
        "from scipy.ndimage import label\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"Albumentations:\", A.__version__)\n",
        "print(\"SMP:\", smp.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_TNBFTQaZZQX",
      "metadata": {
        "id": "_TNBFTQaZZQX"
      },
      "outputs": [],
      "source": [
        "SEED=42\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    print(f\"Random seed fixed at {seed}\")\n",
        "seed_everything(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mm1a0L6zXJhN",
      "metadata": {
        "id": "mm1a0L6zXJhN"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 3 : Prepare Camus Dataset In Desired Directory\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "740F2jR6E4NQ",
      "metadata": {
        "id": "740F2jR6E4NQ"
      },
      "outputs": [],
      "source": [
        "CAMUS_DIRECTORY=Path('<Your Directory>')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "moved-bottle",
      "metadata": {
        "id": "moved-bottle"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 4 : Setup Configurations\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "interim-grant",
      "metadata": {
        "id": "interim-grant"
      },
      "outputs": [],
      "source": [
        "# ====== DATA SETTINGS ======\n",
        "DATA_ROOT = Path(CAMUS_DIRECTORY)\n",
        "USE_VIEWS  = ('2CH', '4CH')        # choose ('2CH',) or ('4CH',) if you prefer one\n",
        "USE_PHASES = ('ED', 'ES')          # for single frames\n",
        "USE_SEQUENCES = False              # True = use *_half_sequence.nii.gz volumes\n",
        "\n",
        "# ====== TRAINING ======\n",
        "DEVICE = 'cuda'\n",
        "EPOCHS = 50\n",
        "LR = 7e-3\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# ====== MODEL ======\n",
        "ENCODER = 'timm-efficientnet-b4'\n",
        "ENCODER_WEIGHTS = 'imagenet'\n",
        "IN_CHANNELS = 1            # grayscale ultrasound\n",
        "NUM_CLASSES = 3            # background, endo, epi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fabulous-peripheral",
      "metadata": {
        "id": "fabulous-peripheral"
      },
      "outputs": [],
      "source": [
        "# split the dataset into training and validation\n",
        "\n",
        "def list_patients(root: Path):\n",
        "    return sorted([p for p in root.glob('patient*') if p.is_dir()])\n",
        "\n",
        "def build_index(root: Path, use_views=('2CH', '4CH'), use_phases=('ED','ES'), use_sequences=False):\n",
        "    rows = []\n",
        "    for pdir in list_patients(root):\n",
        "        pid = pdir.name\n",
        "        for view in use_views:\n",
        "            if use_sequences:\n",
        "                img = pdir / f'{pid}_{view}_half_sequence.nii.gz'\n",
        "                msk = pdir / f'{pid}_{view}_half_sequence_gt.nii.gz'\n",
        "                if img.exists() and msk.exists():\n",
        "                    rows.append(dict(patient=pid, view=view, phase='SEQ',\n",
        "                                     image=str(img), mask=str(msk), is_sequence=True))\n",
        "            else:\n",
        "                for ph in use_phases:\n",
        "                    img = pdir / f'{pid}_{view}_{ph}.nii.gz'\n",
        "                    msk = pdir / f'{pid}_{view}_{ph}_gt.nii.gz'\n",
        "                    if img.exists() and msk.exists():\n",
        "                        rows.append(dict(patient=pid, view=view, phase=ph,\n",
        "                                         image=str(img), mask=str(msk), is_sequence=False))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "df = build_index(DATA_ROOT, USE_VIEWS, USE_PHASES, USE_SEQUENCES)\n",
        "print('Found samples:', len(df))\n",
        "df = df.rename(columns={'image': 'image_path', 'mask': 'mask_path'})\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zxvF4SP6dvuz",
      "metadata": {
        "id": "zxvF4SP6dvuz"
      },
      "outputs": [],
      "source": [
        "patients = sorted(df['patient'].unique())\n",
        "train_p, val_p = train_test_split(patients, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = df[df['patient'].isin(train_p)].reset_index(drop=True)\n",
        "valid_df = df[df['patient'].isin(val_p)].reset_index(drop=True)\n",
        "\n",
        "len(train_df), len(valid_df), len(patients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fancy-knock",
      "metadata": {
        "id": "fancy-knock"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 5 : Augmentation Functions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "possible-cinema",
      "metadata": {
        "id": "possible-cinema"
      },
      "outputs": [],
      "source": [
        "def get_train_augs(image_size=256):\n",
        "    return A.Compose([\n",
        "        # --- Mild geometric jitter (probe placement) ---\n",
        "        A.OneOf([\n",
        "            A.Affine(\n",
        "                scale=(0.90, 1.10),\n",
        "                rotate=(-12, 12),\n",
        "                shear=(-8, 8),\n",
        "                translate_percent=(-0.03, 0.03),\n",
        "                fit_output=False\n",
        "            ),\n",
        "            A.Affine(\n",
        "                scale=(0.95, 1.05),\n",
        "                rotate=(-6, 6),\n",
        "                shear=(-4, 4),\n",
        "                translate_percent=(-0.02, 0.02),\n",
        "                fit_output=False\n",
        "            ),\n",
        "        ], p=0.70),\n",
        "\n",
        "        # --- Local deformations (breathing / coupling) ---\n",
        "        A.OneOf([\n",
        "            A.GridDistortion(num_steps=5, distort_limit=0.20,\n",
        "                             border_mode=cv2.BORDER_REFLECT_101),\n",
        "            A.ElasticTransform(alpha=20, sigma=7,\n",
        "                               border_mode=cv2.BORDER_REFLECT_101),\n",
        "        ], p=0.30),\n",
        "\n",
        "        # --- Cropping/zoom (depth/focus changes) ---\n",
        "        A.RandomResizedCrop(\n",
        "            size=(image_size, image_size),\n",
        "            scale=(0.90, 1.0),\n",
        "            ratio=(0.95, 1.05),\n",
        "            p=0.35\n",
        "        ),\n",
        "\n",
        "        # --- Photometric changes (gain, dynamic range) ---\n",
        "        A.OneOf([\n",
        "            A.RandomBrightnessContrast(brightness_limit=0.25, contrast_limit=0.20),\n",
        "            A.RandomGamma(gamma_limit=(80, 120)),\n",
        "            A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8)),\n",
        "        ], p=0.60),\n",
        "\n",
        "        # --- Ultrasound noise (speckle / sensor) ---\n",
        "        A.OneOf([\n",
        "            A.MultiplicativeNoise(multiplier=(0.85, 1.15)),\n",
        "            A.GaussNoise(std_range=(0.01, 0.05)),\n",
        "        ], p=0.50),\n",
        "\n",
        "        # --- Blur (defocus, motion) ---\n",
        "        A.OneOf([\n",
        "            A.MotionBlur(blur_limit=3),\n",
        "            A.MedianBlur(blur_limit=3),\n",
        "            A.GaussianBlur(blur_limit=3),\n",
        "        ], p=0.30),\n",
        "\n",
        "        # --- Resolution / bandwidth changes ---\n",
        "        A.Downscale(\n",
        "            scale_range=(0.70, 0.95),\n",
        "            p=0.15\n",
        "        ),\n",
        "\n",
        "        # --- Flip ---\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "    ])\n",
        "\n",
        "def get_valid_augs():\n",
        "    return A.Compose([])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceramic-closer",
      "metadata": {
        "id": "ceramic-closer"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 6 : Create Custom Dataset\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rubber-humanitarian",
      "metadata": {
        "id": "rubber-humanitarian"
      },
      "outputs": [],
      "source": [
        "class CamusDataset(Dataset):\n",
        "    def __init__(self, df, augmentations=None, image_size=256, pick_sequence_frame='maxmask'):\n",
        "        \"\"\"\n",
        "        pick_sequence_frame:\n",
        "          - 'random': random frame if a sequence\n",
        "          - 'maxmask': choose frame with largest label area (good default)\n",
        "          - int: fixed index\n",
        "        \"\"\"\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.augs = augmentations\n",
        "        self.size = (image_size, image_size)\n",
        "        self.pick = pick_sequence_frame\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def _load_nii(self, path):\n",
        "        # returns np.ndarray (H, W) or (H, W, T)\n",
        "        arr = nib.load(path).get_fdata()\n",
        "        arr = np.asarray(arr)\n",
        "        return arr\n",
        "\n",
        "    def _choose_frame(self, img, msk):\n",
        "        # img, msk: (H, W, T)\n",
        "        T = img.shape[-1]\n",
        "        if isinstance(self.pick, int):\n",
        "            t = np.clip(self.pick, 0, T-1)\n",
        "        elif self.pick == 'random':\n",
        "            t = np.random.randint(0, T)\n",
        "        else:\n",
        "            # pick frame with max number of labeled pixels\n",
        "            sums = msk.reshape(-1, T).sum(0)\n",
        "            t = int(np.argmax(sums))\n",
        "        return img[..., t], msk[..., t]\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        row = self.df.iloc[index]\n",
        "        img = self._load_nii(str(row.image_path))      # (H, W) or (H, W, T)\n",
        "        msk = self._load_nii(str(row.mask_path)).astype(np.int64)\n",
        "\n",
        "        # if sequence, pick a single frame\n",
        "        if row.is_sequence:\n",
        "            if img.ndim == 2:   # safety\n",
        "                img = img[..., None]\n",
        "                msk = msk[..., None]\n",
        "            img, msk = self._choose_frame(img, msk)\n",
        "\n",
        "        # ensure 2D\n",
        "        if img.ndim == 3 and img.shape[-1] == 1:\n",
        "            img = img[..., 0]\n",
        "        if msk.ndim == 3 and msk.shape[-1] == 1:\n",
        "            msk = msk[..., 0]\n",
        "\n",
        "        # resize (keep masks nearest-neighbor)\n",
        "        img = cv2.resize(img, self.size, interpolation=cv2.INTER_AREA)\n",
        "        msk = cv2.resize(msk, self.size, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # Map mask values >= NUM_CLASSES to 0 (background)\n",
        "        msk[msk >= NUM_CLASSES] = 0\n",
        "\n",
        "        # normalize to [0,1]\n",
        "        img = img.astype(np.float32)\n",
        "        if np.isfinite(img).all():\n",
        "            mn, mx = np.percentile(img, 1), np.percentile(img, 99)\n",
        "            if mx > mn:\n",
        "                img = np.clip((img - mn) / (mx - mn), 0, 1)\n",
        "            else:\n",
        "                img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "        else:\n",
        "            img = np.nan_to_num(img)\n",
        "            img = (img - img.min()) / (img.max() - img.min() + 1e-8)\n",
        "\n",
        "        if self.augs:\n",
        "            out = self.augs(image=img, mask=msk)\n",
        "            img, msk = out['image'], out['mask']\n",
        "\n",
        "        # Assert mask values are within the expected range\n",
        "        assert np.all((msk >= 0) & (msk < NUM_CLASSES)), f\"Mask values out of range: {np.unique(msk)}\"\n",
        "\n",
        "\n",
        "        # to tensors\n",
        "        img = torch.from_numpy(img[None, ...])            # (1, H, W)\n",
        "        msk = torch.from_numpy(msk.astype(np.int64))      # (H, W) with class indices {0,1,2}\n",
        "\n",
        "        return img, msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "looking-lightning",
      "metadata": {
        "id": "looking-lightning"
      },
      "outputs": [],
      "source": [
        "trainset = CamusDataset(train_df, augmentations=get_train_augs(IMAGE_SIZE), image_size=IMAGE_SIZE)\n",
        "validset = CamusDataset(valid_df, augmentations=get_valid_augs(),  image_size=IMAGE_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gothic-extreme",
      "metadata": {
        "id": "gothic-extreme"
      },
      "outputs": [],
      "source": [
        "print(f\"Size of Trainset : {len(trainset)}\")\n",
        "print(f\"Size of Validset : {len(validset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "impossible-literature",
      "metadata": {
        "id": "impossible-literature"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 7 : Load dataset into batches\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "honey-paraguay",
      "metadata": {
        "id": "honey-paraguay"
      },
      "outputs": [],
      "source": [
        "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "validloader = DataLoader(validset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "saved-blend",
      "metadata": {
        "id": "saved-blend"
      },
      "outputs": [],
      "source": [
        "print(f\"Size of Trainloader : {len(trainloader)}\")\n",
        "print(f\"Size of Validloader : {len(validloader)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fiscal-genome",
      "metadata": {
        "id": "fiscal-genome"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 8 : Create Segmentation Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opening-benefit",
      "metadata": {
        "id": "opening-benefit"
      },
      "source": [
        "segmentation_models_pytorch documentation : https://smp.readthedocs.io/en/latest/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "stopped-treaty",
      "metadata": {
        "id": "stopped-treaty"
      },
      "outputs": [],
      "source": [
        "model = smp.Unet(\n",
        "    encoder_name=ENCODER,\n",
        "    encoder_weights=ENCODER_WEIGHTS,\n",
        "    in_channels=IN_CHANNELS,\n",
        "    classes=NUM_CLASSES,\n",
        "    activation=None\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "primary-variance",
      "metadata": {
        "id": "primary-variance"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 9 : Train Model\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "floral-france",
      "metadata": {
        "id": "floral-france"
      },
      "outputs": [],
      "source": [
        "# CrossEntropy expects targets as (H, W) long with class indices\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optional: add Dice loss for multiclass (from_logits=True)\n",
        "try:\n",
        "    dice_loss = smp.losses.DiceLoss(mode='multiclass', from_logits=True)\n",
        "    def loss_fn(pred, target):\n",
        "        return criterion(pred, target) + 0.5 * dice_loss(pred, target)\n",
        "except:\n",
        "    def loss_fn(pred, target):\n",
        "        return criterion(pred, target)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14e7839d",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_epoch(model, loader, optimizer):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for imgs, masks in tqdm(loader):\n",
        "        imgs  = imgs.to(DEVICE)            # (B, 1, H, W)\n",
        "        masks = masks.to(DEVICE)           # (B, H, W) with values in {0,1,2}\n",
        "        assert torch.all((masks >= 0) & (masks < NUM_CLASSES)), f\"Mask values out of range before loss calculation: {torch.unique(masks)}\"\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs)               # (B, 3, H, W)\n",
        "        loss = loss_fn(logits, masks)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total += loss.item() * imgs.size(0)\n",
        "    return total / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5d82922",
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def validate(model, loader):\n",
        "    model.eval()\n",
        "    total = 0.0\n",
        "    for imgs, masks in tqdm(loader):\n",
        "        imgs  = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE)\n",
        "        logits = model(imgs)\n",
        "        loss = loss_fn(logits, masks)\n",
        "        total += loss.item() * imgs.size(0)\n",
        "    return total / len(loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mounted-sword",
      "metadata": {
        "id": "mounted-sword"
      },
      "outputs": [],
      "source": [
        "best_val = 1e9\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    tr = train_one_epoch(model, trainloader, optimizer)\n",
        "    vl = validate(model, validloader)\n",
        "    print(f'Epoch {epoch:03d} | train {tr:.4f} | valid {vl:.4f}')\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), 'best_camus.pth')\n",
        "        print(\"SAVED-MODEL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CVVzhk0HupfA",
      "metadata": {
        "id": "CVVzhk0HupfA"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 10 : Evaluation (with and withoud post-processing) Before K-Fold Calibration\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HHHDZtdbStEP",
      "metadata": {
        "id": "HHHDZtdbStEP"
      },
      "outputs": [],
      "source": [
        "CAVITY_CLASS = 1  # LV cavity class id\n",
        "\n",
        "def clean_cavity_mask(pred_labels, cavity_class=1, kernel_size=3, median_ks=3, connectivity=8):\n",
        "    \"\"\"Keep largest LV cavity component and smooth edges.\"\"\"\n",
        "    cav = (pred_labels == cavity_class).astype(np.uint8)\n",
        "\n",
        "    # Connected components\n",
        "    num_labels, labels_im = cv2.connectedComponents(cav, connectivity=connectivity)\n",
        "    if num_labels > 1:\n",
        "        sizes = [(labels_im == i).sum() for i in range(1, num_labels)]\n",
        "        largest = 1 + int(np.argmax(sizes))\n",
        "        cav = (labels_im == largest).astype(np.uint8)\n",
        "\n",
        "    # Morphological closing (optional)\n",
        "    if kernel_size and kernel_size > 1:\n",
        "        kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
        "        cav = cv2.morphologyEx(cav, cv2.MORPH_CLOSE, kernel)\n",
        "\n",
        "    # Median smoothing (optional)\n",
        "    if median_ks and median_ks > 1:\n",
        "        cav = cv2.medianBlur(cav, median_ks)\n",
        "\n",
        "    # Return labels again (0 bg, cavity_class for LV)\n",
        "    out = np.zeros_like(pred_labels, dtype=np.uint8)\n",
        "    out[cav.astype(bool)] = cavity_class\n",
        "    return out\n",
        "\n",
        "def _principal_axis_length(mask_bin: np.ndarray) -> float:\n",
        "    ys, xs = np.nonzero(mask_bin.astype(bool))\n",
        "    if xs.size < 10: return float('nan')\n",
        "    X = np.stack([xs, ys], axis=1).astype(np.float64)\n",
        "    Xc = X - X.mean(axis=0, keepdims=True)\n",
        "    _, _, Vt = np.linalg.svd(Xc, full_matrices=False)\n",
        "    axis1 = Vt[0]; proj = Xc @ axis1\n",
        "    return float(proj.max() - proj.min())\n",
        "\n",
        "def _area_pixels(mask_bin: np.ndarray) -> float:\n",
        "    return float(mask_bin.astype(np.uint8).sum())\n",
        "\n",
        "def _vol_biplane(A2, L2, A4, L4):\n",
        "    vals = []\n",
        "    if np.isfinite(A2) and np.isfinite(L2) and L2 > 0: vals.append(8.0/(3.0*pi) * (A2*A2)/L2)\n",
        "    if np.isfinite(A4) and np.isfinite(L4) and L4 > 0: vals.append(8.0/(3.0*pi) * (A4*A4)/L4)\n",
        "    if all(np.isfinite([A2,L2,A4,L4])) and L2>0 and L4>0:\n",
        "        Lavg = 0.5*(L2 + L4); vals.append(8.0/(3.0*pi) * (A2*A4)/Lavg)\n",
        "    return float(vals[-1] if len(vals)==3 else (np.mean(vals) if vals else float('nan')))\n",
        "\n",
        "@torch.no_grad()\n",
        "def _predict_one(model, ds, row, device='cuda', use_postproc=False,\n",
        "                 kernel_size=3, median_ks=3, connectivity=8, return_img=False):\n",
        "    img, _ = ds[row.name]                      # (1,H,W)\n",
        "    img_np = img.squeeze(0).cpu().numpy()      # (H,W) for overlay\n",
        "    logits = model(img[None].to(device))\n",
        "    pred = torch.argmax(logits, dim=1)[0].cpu().numpy()  # labels\n",
        "\n",
        "    if use_postproc:\n",
        "        pred_use = clean_cavity_mask(pred, cavity_class=CAVITY_CLASS,\n",
        "                                     kernel_size=kernel_size, median_ks=median_ks, connectivity=connectivity)\n",
        "    else:\n",
        "        pred_use = pred\n",
        "\n",
        "    cav = (pred_use == CAVITY_CLASS).astype(np.uint8)\n",
        "    A = _area_pixels(cav)\n",
        "    L = _principal_axis_length(cav)\n",
        "    return (A, L, pred_use, img_np) if return_img else (A, L, pred_use)\n",
        "\n",
        "def parse_cfg_numeric_lines(path: Path):\n",
        "    out = {}\n",
        "    if not path.exists(): return out\n",
        "    for line in path.read_text(errors='ignore').splitlines():\n",
        "        line = line.strip()\n",
        "        if not line or line.startswith('#'): continue\n",
        "        if ':' in line: k, v = line.split(':', 1)\n",
        "        elif '=' in line: k, v = line.split('=', 1)\n",
        "        else: continue\n",
        "        k = k.strip().lower(); v = v.strip()\n",
        "        num = None\n",
        "        for tok in v.replace(',', ' ').split():\n",
        "            try: num = float(tok); break\n",
        "            except: pass\n",
        "        if num is not None: out[k] = num\n",
        "    return out\n",
        "\n",
        "def read_gt_ef_from_patient_dir(pdir: Path, prefer='4ch'):\n",
        "    order = ['4ch','2ch'] if prefer.lower()=='4ch' else ['2ch','4ch']\n",
        "    files = {lab: pdir / f'Info_{lab.upper()}.cfg' for lab in ['2ch','4ch']}\n",
        "    infos = {lab: parse_cfg_numeric_lines(files[lab]) for lab in files}\n",
        "    for lab in order:\n",
        "        ef = infos.get(lab, {}).get('ef', None)\n",
        "        if ef is not None: return float(ef)\n",
        "    for lab in order:\n",
        "        edv = infos.get(lab, {}).get('edv', None)\n",
        "        esv = infos.get(lab, {}).get('esv', None)\n",
        "        if edv and esv and edv > 0: return (edv - esv)/edv*100.0\n",
        "    return float('nan')\n",
        "\n",
        "@torch.no_grad()\n",
        "def ef_from_predictions_for_patient(model, ds, patient_id: str, device='cuda',\n",
        "                                    use_postproc=False, kernel_size=3, median_ks=3, connectivity=8,\n",
        "                                    return_thumbs=False):\n",
        "    sub = ds.df[ds.df['patient'] == patient_id]\n",
        "    if sub.empty:\n",
        "        return {\"patient\": patient_id, \"EF_pred_%\": float('nan'), \"EF_gt_%\": float('nan')}\n",
        "\n",
        "    pdir = Path(sub.iloc[0]['image_path']).parent\n",
        "    EF_gt = read_gt_ef_from_patient_dir(pdir)\n",
        "\n",
        "    A2_ED=L2_ED=A4_ED=L4_ED = np.nan\n",
        "    A2_ES=L2_ES=A4_ES=L4_ES = np.nan\n",
        "    thumbs = []\n",
        "\n",
        "    for view, ph, lab in [('2CH','ED','2CH ED'), ('2CH','ES','2CH ES'),\n",
        "                          ('4CH','ED','4CH ED'), ('4CH','ES','4CH ES')]:\n",
        "        r = sub[(sub['view']==view) & (sub['phase']==ph)]\n",
        "        if len(r):\n",
        "            vals = _predict_one(model, ds, r.iloc[0], device,\n",
        "                                use_postproc=use_postproc,\n",
        "                                kernel_size=kernel_size, median_ks=median_ks, connectivity=connectivity,\n",
        "                                return_img=return_thumbs)\n",
        "            if return_thumbs:\n",
        "                A, L, pred_use, img_np = vals\n",
        "                thumbs.append((lab, img_np, pred_use))\n",
        "            else:\n",
        "                A, L, pred_use = vals\n",
        "\n",
        "            if ph=='ED':\n",
        "                if view=='2CH': A2_ED, L2_ED = A, L\n",
        "                else:           A4_ED, L4_ED = A, L\n",
        "            else:\n",
        "                if view=='2CH': A2_ES, L2_ES = A, L\n",
        "                else:           A4_ES, L4_ES = A, L\n",
        "\n",
        "    EDV = _vol_biplane(A2_ED, L2_ED, A4_ED, L4_ED)\n",
        "    ESV = _vol_biplane(A2_ES, L2_ES, A4_ES, L4_ES)\n",
        "    EF_pred = (EDV - ESV)/EDV*100.0 if (np.isfinite(EDV) and np.isfinite(ESV) and EDV>0) else float('nan')\n",
        "\n",
        "    out = {\n",
        "        \"patient\": patient_id,\n",
        "        \"EF_pred_%\": EF_pred,\n",
        "        \"EF_gt_%\": EF_gt,\n",
        "        \"EF_error_%\": (EF_pred - EF_gt) if (np.isfinite(EF_pred) and np.isfinite(EF_gt)) else float('nan'),\n",
        "        \"EDV_px3\": EDV, \"ESV_px3\": ESV,\n",
        "    }\n",
        "    if return_thumbs: out['thumbs'] = thumbs\n",
        "    return out\n",
        "def eval_val_split(use_postproc=False, kernel_size=3, median_ks=3, connectivity=8):\n",
        "    val_patients = sorted(valid_df['patient'].unique())\n",
        "    ef_rows = []\n",
        "    for pid in tqdm(val_patients):\n",
        "        out = ef_from_predictions_for_patient(\n",
        "            model, validset, pid, device=DEVICE,\n",
        "            use_postproc=use_postproc,\n",
        "            kernel_size=kernel_size, median_ks=median_ks, connectivity=connectivity,\n",
        "            return_thumbs=False  # set True only if you want overlays back\n",
        "        )\n",
        "        ef_rows.append(out)\n",
        "\n",
        "    ef_df = pd.DataFrame(ef_rows)\n",
        "    pred = ef_df['EF_pred_%'].to_numpy()\n",
        "    gt   = ef_df['EF_gt_%'].to_numpy()\n",
        "    err  = pred - gt\n",
        "\n",
        "    # Summary\n",
        "    mae  = np.mean(np.abs(err))\n",
        "    bias = np.mean(err)\n",
        "    sd   = np.std(err, ddof=1)\n",
        "    loa1, loa2 = bias - 1.96*sd, bias + 1.96*sd\n",
        "    print(f\"[postproc={use_postproc}]  MAE={mae:.2f} | Bias={bias:.2f} | SD={sd:.2f} | LoA=[{loa1:.2f}, {loa2:.2f}]\")\n",
        "\n",
        "    # Correlation\n",
        "    r, p = pearsonr(gt, pred); R2 = r**2\n",
        "    print(f\"Pearson r = {r:.3f} (p={p:.2e}), R² = {R2:.3f}\")\n",
        "\n",
        "    # Bootstrap 95% CI for MAE & Bias\n",
        "    rng = np.random.default_rng(42)\n",
        "    B = 2000\n",
        "    idx = rng.integers(0, len(err), (B, len(err)))\n",
        "    mae_ci  = np.percentile(np.mean(np.abs(err[idx]), axis=1), [2.5, 97.5])\n",
        "    bias_ci = np.percentile(np.mean(err[idx], axis=1), [2.5, 97.5])\n",
        "    print(f\"MAE 95% CI: [{mae_ci[0]:.2f}%, {mae_ci[1]:.2f}%]\")\n",
        "    print(f\"Bias 95% CI: [{bias_ci[0]:.2f}%, {bias_ci[1]:.2f}%]\")\n",
        "\n",
        "    # Scatter\n",
        "    plt.figure(figsize=(5,5))\n",
        "    lims = [min(gt.min(), pred.min())-2, max(gt.max(), pred.max())+2]\n",
        "    plt.scatter(gt, pred, s=16)\n",
        "    plt.plot(lims, lims, '--')\n",
        "    plt.xlabel('GT EF (%)'); plt.ylabel('Pred EF (%)')\n",
        "    plt.title(f'Pred vs GT (R²={R2:.2f})')\n",
        "    plt.xlim(lims); plt.ylim(lims); plt.show()\n",
        "\n",
        "    # Bland–Altman\n",
        "    plt.figure(figsize=(6,4))\n",
        "    mean_ef = (pred + gt) / 2\n",
        "    plt.scatter(mean_ef, err, s=16)\n",
        "    plt.axhline(bias, linestyle='--', label=f\"bias={bias:.2f}%\")\n",
        "    plt.axhline(loa1, color='r', linestyle=':', label=f\"LoA={loa1:.2f}%\")\n",
        "    plt.axhline(loa2, color='r', linestyle=':')\n",
        "    plt.xlabel('Mean of Pred & GT EF (%)'); plt.ylabel('Pred − GT EF (%)')\n",
        "    plt.title('Bland–Altman: EF')\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "    return ef_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8OxfhAP6TXmc",
      "metadata": {
        "id": "8OxfhAP6TXmc"
      },
      "outputs": [],
      "source": [
        "# Run raw (no post-processing)\n",
        "ef_df_raw = eval_val_split(use_postproc=False)\n",
        "\n",
        "# Run with post-processing (largest component + morphology)\n",
        "ef_df_pp  = eval_val_split(use_postproc=True, kernel_size=3, median_ks=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rDJ2qRIKptIM",
      "metadata": {
        "id": "rDJ2qRIKptIM"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 11 : Calibration And Reevaluation\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "o-TXsBFxdPM0",
      "metadata": {
        "id": "o-TXsBFxdPM0"
      },
      "outputs": [],
      "source": [
        "def clip_ef(arr, lo=5.0, hi=85.0):\n",
        "    return np.clip(arr, lo, hi)\n",
        "\n",
        "def kfold_calibration_df(df_in, K=5, seed=42):\n",
        "    df = df_in.copy()\n",
        "    df['EF_pred_cal_%'] = np.nan\n",
        "    kf = KFold(n_splits=K, shuffle=True, random_state=seed)\n",
        "    for tr, te in kf.split(df):\n",
        "        x = df.iloc[tr]['EF_pred_%'].to_numpy()\n",
        "        y = df.iloc[tr]['EF_gt_%'].to_numpy()\n",
        "        a, b = np.polyfit(x, y, 1)\n",
        "        df.loc[df.index[te], 'EF_pred_cal_%'] = clip_ef(a*df.iloc[te]['EF_pred_%'].to_numpy() + b)\n",
        "    return df\n",
        "\n",
        "def summarize(pred, gt):\n",
        "    e = pred - gt\n",
        "    mae = np.mean(np.abs(e))\n",
        "    bias = np.mean(e)\n",
        "    sd = np.std(e, ddof=1)\n",
        "    loa1, loa2 = bias - 1.96*sd, bias + 1.96*sd\n",
        "    r, p = pearsonr(gt, pred)\n",
        "    R2 = r**2\n",
        "    # bootstrap 95% CI for MAE & bias\n",
        "    B = 2000\n",
        "    rng = np.random.default_rng(42)\n",
        "    idx = rng.integers(0, len(e), (B, len(e)))\n",
        "    mae_ci  = np.percentile(np.mean(np.abs(e[idx]), axis=1), [2.5, 97.5])\n",
        "    bias_ci = np.percentile(np.mean(e[idx], axis=1), [2.5, 97.5])\n",
        "    return dict(MAE=mae, Bias=bias, SD=sd, LoA=(loa1, loa2), r=r, R2=R2,\n",
        "                MAE_CI=mae_ci, Bias_CI=bias_ci)\n",
        "\n",
        "# K-fold calibration\n",
        "df_cal = kfold_calibration_df(ef_df_pp, K=5, seed=SEED)\n",
        "\n",
        "# Metrics BEFORE and AFTER\n",
        "pre = summarize(ef_df_pp['EF_pred_%'].to_numpy(),      ef_df_pp['EF_gt_%'].to_numpy())\n",
        "post= summarize(df_cal['EF_pred_cal_%'].to_numpy(), df_cal['EF_gt_%'].to_numpy())\n",
        "\n",
        "print(\"Before calib : \"\n",
        "      f\"MAE={pre['MAE']:.2f} [{pre['MAE_CI'][0]:.2f},{pre['MAE_CI'][1]:.2f}]  | \"\n",
        "      f\"Bias={pre['Bias']:.2f} [{pre['Bias_CI'][0]:.2f},{pre['Bias_CI'][1]:.2f}]  | \"\n",
        "      f\"SD={pre['SD']:.2f}  | LoA=[{pre['LoA'][0]:.2f},{pre['LoA'][1]:.2f}]  | \"\n",
        "      f\"r={pre['r']:.3f}, R²={pre['R2']:.3f}\")\n",
        "\n",
        "print(\"After  calib : \"\n",
        "      f\"MAE={post['MAE']:.2f} [{post['MAE_CI'][0]:.2f},{post['MAE_CI'][1]:.2f}] | \"\n",
        "      f\"Bias={post['Bias']:.2f} [{post['Bias_CI'][0]:.2f},{post['Bias_CI'][1]:.2f}] | \"\n",
        "      f\"SD={post['SD']:.2f} | LoA=[{post['LoA'][0]:.2f},{post['LoA'][1]:.2f}] | \"\n",
        "      f\"r={post['r']:.3f}, R²={post['R2']:.3f}\")\n",
        "\n",
        "# Scatter (overlay before vs after)\n",
        "gt    = ef_df_pp['EF_gt_%'].to_numpy()\n",
        "pred0 = ef_df_pp['EF_pred_%'].to_numpy()\n",
        "pred1 = df_cal['EF_pred_cal_%'].to_numpy()\n",
        "\n",
        "plt.figure(figsize=(5.5,5.5))\n",
        "lims = [min(gt.min(), pred0.min(), pred1.min())-2, max(gt.max(), pred0.max(), pred1.max())+2]\n",
        "plt.scatter(gt, pred0, s=14, label='Before')\n",
        "plt.scatter(gt, pred1, s=14, alpha=0.8, label='After (calibrated)')\n",
        "plt.plot(lims, lims, '--', linewidth=1)\n",
        "plt.xlabel('GT EF (%)'); plt.ylabel('Pred EF (%)')\n",
        "plt.title(f'Pred vs GT (After: R²={post[\"R2\"]:.2f}, MAE={post[\"MAE\"]:.1f}%)')\n",
        "plt.xlim(lims); plt.ylim(lims); plt.legend(); plt.show()\n",
        "\n",
        "# Bland–Altman (after calibration)\n",
        "diff = pred1 - gt\n",
        "bias = diff.mean(); sd = diff.std(ddof=1)\n",
        "loa1, loa2 = bias - 1.96*sd, bias + 1.96*sd\n",
        "mean_ef = (pred1 + gt)/2\n",
        "\n",
        "plt.figure(figsize=(6,4.2))\n",
        "plt.scatter(mean_ef, diff, s=16)\n",
        "plt.axhline(bias, linestyle='--', label=f\"bias={bias:.2f}%\")\n",
        "plt.axhline(loa1, color='r', linestyle=':', label=f\"LoA={loa1:.2f}%\")\n",
        "plt.axhline(loa2, color='r', linestyle=':')\n",
        "plt.xlabel('Mean of Pred_cal & GT EF (%)'); plt.ylabel('Pred_cal − GT EF (%)')\n",
        "plt.title('Bland–Altman: EF (calibrated)')\n",
        "plt.legend(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "srnR_utxrDeP",
      "metadata": {
        "id": "srnR_utxrDeP"
      },
      "source": [
        "---\n",
        "\n",
        "# Task 12 : Visualization (Segmentation)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "seventh-seating",
      "metadata": {
        "id": "seventh-seating"
      },
      "outputs": [],
      "source": [
        "def _overlay_rgb(gray_hw, mask_hw, cavity_id=1, color=(1,0,0), alpha=0.35):\n",
        "    g = gray_hw.astype(np.float32)\n",
        "    g = (g - g.min()) / (g.max() - g.min() + 1e-8)\n",
        "    rgb = np.stack([g, g, g], axis=-1)\n",
        "\n",
        "    overlay = rgb.copy()\n",
        "    overlay[mask_hw == cavity_id] = color\n",
        "    out = (1 - alpha) * rgb + alpha * overlay\n",
        "    return out\n",
        "\n",
        "def _get_row_for(sub_df, view, phase):\n",
        "    r = sub_df[(sub_df['view'] == view) & (sub_df['phase'] == phase)]\n",
        "    return None if r.empty else r.iloc[0]\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_patient_ed_es_panel(model, ds, df, patient_id,\n",
        "                             device='cuda',\n",
        "                             use_postproc=True, kernel_size=3, median_ks=3, connectivity=8,\n",
        "                             alpha_gt=0.35, alpha_pred=0.35,\n",
        "                             cavity_class=CAVITY_CLASS,\n",
        "                             figsize=(14, 7)):\n",
        "    \"\"\"\n",
        "    Panel layout (2 rows x 4 cols):\n",
        "      Row 1 (2CH):  ED GT | ED Pred | ES GT | ES Pred\n",
        "      Row 2 (4CH):  ED GT | ED Pred | ES GT | ES Pred\n",
        "\n",
        "    Uses your _predict_one() (no changes) and accesses GT masks via ds[idx].\n",
        "    \"\"\"\n",
        "    sub = df[df['patient'] == patient_id]\n",
        "    if sub.empty:\n",
        "        raise ValueError(f\"No samples for patient '{patient_id}' in dataframe.\")\n",
        "\n",
        "    # Prepare figure\n",
        "    fig, axes = plt.subplots(2, 4, figsize=figsize)\n",
        "\n",
        "    def _one(view, phase, row_idx, col_base):\n",
        "        row = _get_row_for(sub, view, phase)\n",
        "        if row is None:\n",
        "            # blank panels if missing\n",
        "            for k in range(4):\n",
        "                axes[row_idx, col_base+k//2].axis('off')\n",
        "            return\n",
        "\n",
        "        idx = row.name\n",
        "        img_t, gt_mask = ds[idx]          # (1,H,W), (H,W)\n",
        "        img_hw = img_t.squeeze(0).cpu().numpy()\n",
        "\n",
        "        # predict using your helper (returns (A, L, pred_labels, img_np) when return_img=True)\n",
        "        A, L, pred_labels, img_np = _predict_one(\n",
        "            model, ds, row, device=device,\n",
        "            use_postproc=use_postproc,\n",
        "            kernel_size=kernel_size, median_ks=median_ks, connectivity=connectivity,\n",
        "            return_img=True\n",
        "        )\n",
        "\n",
        "        # overlays\n",
        "        gt_overlay   = _overlay_rgb(img_hw, gt_mask, cavity_id=cavity_class,\n",
        "                                    color=(1,0,0), alpha=alpha_gt)\n",
        "        pred_overlay = _overlay_rgb(img_hw, pred_labels, cavity_id=cavity_class,\n",
        "                                    color=(0,1,0), alpha=alpha_pred)\n",
        "\n",
        "        # ED -> columns [0,1], ES -> columns [2,3]\n",
        "        col_off = 0 if phase == 'ED' else 2\n",
        "\n",
        "        axes[row_idx, col_off+0].imshow(gt_overlay);   axes[row_idx, col_off+0].set_title(f'{view} {phase} – GT')\n",
        "        axes[row_idx, col_off+1].imshow(pred_overlay); axes[row_idx, col_off+1].set_title(f'{view} {phase} – Pred')\n",
        "\n",
        "        axes[row_idx, col_off+0].axis('off'); axes[row_idx, col_off+1].axis('off')\n",
        "\n",
        "    # Top row: 2CH, Bottom row: 4CH\n",
        "    _one('2CH', 'ED', row_idx=0, col_base=0)\n",
        "    _one('2CH', 'ES', row_idx=0, col_base=0)\n",
        "    _one('4CH', 'ED', row_idx=1, col_base=0)\n",
        "    _one('4CH', 'ES', row_idx=1, col_base=0)\n",
        "\n",
        "    # Optional: include EF info using your ef_from_predictions_for_patient()\n",
        "    try:\n",
        "        out = ef_from_predictions_for_patient(model, ds, patient_id, device=device,\n",
        "                                              use_postproc=use_postproc,\n",
        "                                              kernel_size=kernel_size, median_ks=median_ks,\n",
        "                                              connectivity=connectivity, return_thumbs=False)\n",
        "        ef_pred = out.get(\"EF_pred_%\", np.nan)\n",
        "        ef_gt   = out.get(\"EF_gt_%\",   np.nan)\n",
        "        fig.suptitle(f\"Patient {patient_id}  |  EF (GT)={ef_gt:.1f}%   EF (Pred)={ef_pred:.1f}%\", fontsize=12)\n",
        "    except Exception:\n",
        "        fig.suptitle(f\"Patient {patient_id}\", fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "@torch.no_grad()\n",
        "def plot_random_patients_ed_es(model, ds, df, n=4, seed=42, **panel_kwargs):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    patients = sorted(df['patient'].unique())\n",
        "    if len(patients) == 0:\n",
        "        raise ValueError(\"No patients found in df.\")\n",
        "    pick = rng.choice(patients, size=min(n, len(patients)), replace=False)\n",
        "\n",
        "    figs = []\n",
        "    for pid in pick:\n",
        "        fig = plot_patient_ed_es_panel(model, ds, df, str(pid), **panel_kwargs)\n",
        "        figs.append(fig)\n",
        "        plt.show()   # show each panel\n",
        "    return figs\n",
        "_ = plot_random_patients_ed_es(\n",
        "    model, validset, valid_df, n=6, seed=SEED,\n",
        "    device=DEVICE, use_postproc=True, alpha_gt=0.40, alpha_pred=0.40\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rRTW0Zd2WXkG",
      "metadata": {
        "id": "rRTW0Zd2WXkG"
      },
      "source": [
        "\n",
        "\n",
        "Linkedin : https://www.linkedin.com/in/khaled-elrefaey/"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
